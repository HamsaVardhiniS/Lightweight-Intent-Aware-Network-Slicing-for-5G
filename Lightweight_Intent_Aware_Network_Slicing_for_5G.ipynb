{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lho3bzh7pe4Q"
   },
   "source": [
    "# Lightweight Intent-Aware Network Slicing for 5G\n",
    "\n",
    "**Team:** Hamsavardhini S (23PD13)  \n",
    "\n",
    "## Project Overview\n",
    "5G networks face significant challenges in efficiently allocating resources and managing slice admission due to dynamic traffic patterns, heterogeneous service requirements (eMBB, URLLC, mIoT), and strict Quality of Service (QoS) and Service Level Agreement (SLA) constraints. Traditional slicing mechanisms often lack real-time adaptability and generalization across different network topologies.  \n",
    "\n",
    "This project presents a **lightweight, AI-driven network slicing framework** that enables **intent-aware, real-time slicing decisions**. The framework integrates multiple AI components:\n",
    "\n",
    "1. **Natural Language Processing (NLP)** – Extracts user intent and QoS requirements from textual service requests.  \n",
    "2. **Graph Neural Networks (GNN)** – Predicts slice-level KPIs such as latency, jitter, and packet loss.  \n",
    "3. **Reinforcement Learning (RL)** – Performs adaptive resource allocation and slice admission control.  \n",
    "4. **Explainable AI (XAI)** – Provides transparency for GNN predictions and RL decisions.  \n",
    "\n",
    "## Key Objectives\n",
    "- Translate natural language service requests into structured slice requirements.\n",
    "- Simulate network topologies, traffic patterns, and dynamic slice states.\n",
    "- Predict slice KPIs accurately for informed resource allocation.\n",
    "- Enable real-time slice admission and bandwidth allocation using RL agents.\n",
    "- Ensure SLA compliance and provide interpretable explanations for network decisions.\n",
    "\n",
    "## Workflow Summary\n",
    "1. **Module 1: Intent Parser (NLP Layer)** – Converts user requests into structured JSON slice intents.  \n",
    "2. **Module 2: Topology & Traffic Simulator** – Builds heterogeneous network graphs and simulates dynamic traffic.  \n",
    "3. **Module 3: KPI Predictor (GNN)** – Predicts slice KPIs using a lightweight graph neural network.  \n",
    "4. **Module 4: Slice Admission & Resource Allocation (RL Agent)** – Makes real-time decisions on slice acceptance and resource assignment.  \n",
    "5. **Module 5: Slice Lifecycle Manager** – Monitors active slices, triggers scaling, and logs performance metrics.  \n",
    "6. **Module 6: Explainability Layer** – Provides interpretable insights into GNN predictions and RL decisions.\n",
    "\n",
    "## Evaluation Metrics\n",
    "- SLA Satisfaction & Delay Satisfaction Ratio  \n",
    "- Bandwidth Utilization  \n",
    "- Fairness Index (Jain’s)  \n",
    "- Admission Rate & Policy Efficiency Score  \n",
    "- Detection of Under-/Over-Provisioning  \n",
    "\n",
    "This notebook documents the full implementation workflow of the framework, along with simulation results, KPI predictions, and evaluation outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bApXeFl8hesL"
   },
   "source": [
    "*Install and import Dependencies*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7avK99PFB68"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --force-reinstall \"numpy==1.26.4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4uqmU9gRgfQC"
   },
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision torchaudio torch-geometric dgl\n",
    "!pip install -q scikit-learn networkx\n",
    "!pip install -q transformers sentencepiece captum\n",
    "!pip install -q gymnasium stable-baselines3\n",
    "!pip install -q matplotlib seaborn\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1dK_YfEcW-u"
   },
   "outputs": [],
   "source": [
    "import re, json, math, random, time\n",
    "import numpy as np\n",
    "import torch, shap, spacy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "import gymnasium as gym\n",
    "\n",
    "from collections import defaultdict\n",
    "from transformers import pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import HeteroConv, GATConv\n",
    "from captum.attr import IntegratedGradients\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9Hu4F_JdK3X"
   },
   "source": [
    "## Module 1: Intent Parser (LLM-Based)\n",
    "\n",
    "**Purpose:**  \n",
    "Convert a natural language service request into a structured JSON object representing 5G slice requirements. This allows the slicing framework to interpret user intent and QoS needs in a machine-readable format.\n",
    "\n",
    "**Input:**  \n",
    "- Natural language request string, e.g.,  \n",
    "  _\"low-latency video streaming for 1000 users with 10ms delay and 5ms jitter\"_\n",
    "\n",
    "**Process:**  \n",
    "1. Use a Large Language Model (LLM) to extract key slice parameters.  \n",
    "2. Extract the following fields: `slice_type` (eMBB, URLLC, mIoT), `latency` (ms), `jitter` (ms), `bandwidth` (Mbps), `priority` (low, medium, high), and `user_count`.  \n",
    "3. Apply fallback rules if the LLM output is malformed:  \n",
    "   - Detect keywords (e.g., \"video\" → eMBB, \"iot\" → mIoT, \"latency\" → URLLC).  \n",
    "   - Extract numeric values from text for latency, jitter, and user count.  \n",
    "   - Assign default bandwidth and priority based on heuristics.\n",
    "\n",
    "**Output:**  \n",
    "Structured JSON object representing slice intent. Example:  \n",
    "```json\n",
    "{\n",
    "  \"slice_type\": \"eMBB\",\n",
    "  \"latency\": 10,\n",
    "  \"jitter\": 5,\n",
    "  \"bandwidth\": 100,\n",
    "  \"priority\": \"high\",\n",
    "  \"user_count\": 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397,
     "referenced_widgets": [
      "c923e7c10853479bb2181a379b4745d4",
      "22d9899e9560438eb71ddd508cef8b5b",
      "6a7e8280be734c029c3223b4dcd69557",
      "fdd337382a0644b4a2c93327a713fa4e",
      "ad5d776052ac49f299e98cbe32db51a2",
      "4200118f15764e46bad1fae404d8b608",
      "7bc16a91739d47a1b9b60e43207a317f",
      "625007c7ff3440a3b4734482428612f9",
      "622f287bccb749fa820d257877ffe6e7",
      "3ae91d82d03e45e1a6cf1e7660a50935",
      "3b51368d7d634b6f8d029de1672eae77",
      "116ea1128ff14d1faaf03213ad406a13",
      "314837bba85548d29094a353315efaa2",
      "7bf755305c744481abad9a5fff027cfa",
      "48fa34e27e564376b2e4424589f94833",
      "e9e99fffd53b4e93a8519092ef57c250",
      "5fdeaf9551f74a2ba561333805858e46",
      "10e80d47a5a34e748ff1636d2f0d279d",
      "aebbf1f88c0b4c32a559c259de951400",
      "5400140401f54bb89c36c0d614bd0a4a",
      "2126446fdb444e36a1dead2268a0488e",
      "4656c909a84349fbad8d30c69b205cb3",
      "5dba5f04ebf641a8b1819ae6c037efdc",
      "3e5f81beedbd49609902a762ff937b52",
      "6436221b8eb04ece89809ef9a8a8983d",
      "f9d38128645a486d98bd14828c45aa7b",
      "f90d85f48245403c81042a73054dae5f",
      "3d761290ac204653a52f94d8090b76d3",
      "c69f422863da4d6b9fde46cc43f200ee",
      "2d5d37bdc49849f4ae32b3bae7023071",
      "23e8803a131a4c15958ab8ef053c943d",
      "6c1067e47c3248d1bab5b838e1fca4a6",
      "45758489c51c4ed58ece9c050344d73b",
      "2b79ee2f284947aeb27cdab533f9a6b5",
      "6b4d022ae5254a498e89b16df3cdafeb",
      "2bc42fae99ad4ef9aba8300ae0c3f003",
      "41cf5e6cf2c64b05a2f624d556629a53",
      "a97399b184a34b56953293200169abc6",
      "6b9ab553e9114f36a5e275a5cc3d2e13",
      "4553edaebcb74b7cbeca62ac02977a22",
      "7ce494a79ccb4bcb90a88d95d12d4d5f",
      "67201b8082cb435b8d133eafa70e10b9",
      "51067274c0a945b790805bafcee15c6f",
      "80f76f596a3246c59666f0ecfd1367ac",
      "0fcb07d8f0be489fad84fad3a9f47e18",
      "21458983f3734110b05a7c289774004b",
      "78b3733a461f4badbce936859c0774ad",
      "2ff19673940346b1ada62afefacbeee7",
      "58deb667f8f248ad9f4e7d47ecee6b71",
      "1b80bb4171504b32a7785375549ce735",
      "84753c174eb347d7a996fa6afe9c6318",
      "d5bba88a857a4048ba4ca18063e810b1",
      "b717aadcb331425d931e851ed8e94051",
      "1ee27252a8e240248d142d58c2dd1675",
      "5c05b342762a4a048ac4e57bf748337f",
      "2198012cf4b14c84ae6299259db518a4",
      "c514935bab9447abbfbd8a5b71361bea",
      "daac3e7d2069488199fdaddf49f2800d",
      "ede2e612b916469da321559158da3977",
      "ecf3f32f248b4d40864221525c0b8c74",
      "388f654c69f941acb309e3aa3d41f366",
      "3e5f498747634555bb3bd10a2608addf",
      "cc6713d123e04dcb96d0d37e8bb773f1",
      "88c11456fb8a4dc7a526235923318976",
      "5fd8142bac404c36bdf70d4a112850a8",
      "89a70ee6d9064167b0902ed4bdf3d145",
      "d481ddd26c0a4555b970daa6d5dca263",
      "6db8b02560944d709b020a4c30b23104",
      "ea7d7829527e4384b083dfe5b084ca66",
      "546c785b3d9048469424799c78965452",
      "bb44ad03c47c41ee87d0242756f36d45",
      "7f1b55b4207645e9bf55d0173c36b8b1",
      "a58a1bc0fc204ca6ac5c0921b3725a6f",
      "d1c9a8d2d22948e780ca2943db57e4ed",
      "427eade96c6243be956bd04e57766665",
      "ba9f22de0afb4471bf66e27547d1c27e",
      "5e5f4ef96c0442189f5c8382d5772c11"
     ]
    },
    "id": "5UWTHdWCcO6t",
    "outputId": "db7e93ba-3241-4241-e7dd-e05d82d08c41"
   },
   "outputs": [],
   "source": [
    "llm_parser = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\", max_new_tokens=128)\n",
    "\n",
    "def parse_intent_llm(request_text):\n",
    "    prompt = f\"\"\"\n",
    "      You are an expert intent parser for 5G slicing.\n",
    "      Extract the following fields and return only a JSON object:\n",
    "\n",
    "      - slice_type (eMBB, URLLC, mIoT)\n",
    "      - latency (in ms)\n",
    "      - jitter (in ms)\n",
    "      - bandwidth (in Mbps)\n",
    "      - priority (low, medium, high)\n",
    "      - user_count (number of users)\n",
    "\n",
    "      Example:\n",
    "      Request: \"low-latency video streaming for 1000 users with 10ms delay and 5ms jitter\"\n",
    "      Output: {{\"slice_type\": \"eMBB\", \"latency\": 10, \"jitter\": 5, \"bandwidth\": 100, \"priority\": \"high\", \"user_count\": 1000}}\n",
    "\n",
    "      Now parse:\n",
    "      Request: \"{request_text}\"\n",
    "      Output:\n",
    "      \"\"\"\n",
    "    response = llm_parser(prompt)[0][\"generated_text\"]\n",
    "\n",
    "    match = re.search(r\"\\{.*\\}\", response, re.DOTALL)\n",
    "    if match:\n",
    "        json_str = match.group(0)\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "    intent = {}\n",
    "    t = request_text.lower()\n",
    "    if \"video\" in t: intent[\"slice_type\"] = \"eMBB\"\n",
    "    elif \"iot\" in t: intent[\"slice_type\"] = \"mIoT\"\n",
    "    else: intent[\"slice_type\"] = \"URLLC\" if \"latency\" in t else \"eMBB\"\n",
    "    intent[\"latency\"] = int(re.search(r\"(\\d+)ms delay\", t).group(1)) if \"delay\" in t else 50\n",
    "    intent[\"jitter\"] = int(re.search(r\"(\\d+)ms jitter\", t).group(1)) if \"jitter\" in t else 5\n",
    "    intent[\"user_count\"] = int(re.search(r\"(\\d+) users\", t).group(1)) if \"users\" in t else 100\n",
    "    intent[\"bandwidth\"] = 100\n",
    "    intent[\"priority\"] = \"high\" if \"low-latency\" in t else \"medium\"\n",
    "\n",
    "    return intent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wi6EhZMGppsO"
   },
   "source": [
    "## Module 2: Topology, Traffic Simulator & Resource State Generator\n",
    "\n",
    "**Purpose:**  \n",
    "Simulate a 5G network environment by generating network topologies, traffic flows, and resource states. Prepares heterogeneous graph representations and KPI metrics for downstream GNN-based predictions and RL-based slice management.\n",
    "\n",
    "**Input:**  \n",
    "- Structured intent JSON from Module 1  \n",
    "- Number of network nodes (optional)  \n",
    "- Random seed for reproducibility (optional)  \n",
    "- Routing strategy (e.g., shortest path, ECMP)\n",
    "\n",
    "**Process:**  \n",
    "1. **Topology Generation:**  \n",
    "   - Create synthetic network topologies using a directed multigraph.  \n",
    "   - Assign routers as nodes and links as edges with attributes like bandwidth, base latency, buffer size, and ports.  \n",
    "   - Optionally load a real-world network graph.\n",
    "\n",
    "2. **Traffic Derivation:**  \n",
    "   - Generate flows for each slice based on user count and bandwidth in the intent JSON.  \n",
    "   - Simulate multiple flows per slice with time-varying bandwidth to capture burstiness.  \n",
    "   - Assign slice-specific parameters (e.g., buffer size, service rate factors) based on slice type (eMBB, URLLC, mIoT).\n",
    "\n",
    "3. **Routing & KPI Calculation:**  \n",
    "   - Compute routing paths using shortest-path or ECMP algorithms.  \n",
    "   - Calculate per-flow metrics:  \n",
    "     - **Delay:** sum of base link latency + queueing delay (MM1 approximation).  \n",
    "     - **Jitter:** standard deviation of time-varying delays.  \n",
    "     - **Packet loss:** percentage of lost packets based on service rate vs arrival rate.  \n",
    "     - **Delta:** difference between target latency and achieved path delay.  \n",
    "   - Compile flow-level metrics including priority, slice type, and user count.\n",
    "\n",
    "4. **Heterogeneous Graph Construction:**  \n",
    "   - Build a graph with node types: `link`, `queue`, `flow`.  \n",
    "   - Connect nodes via edges representing dependencies: `link->flow`, `queue->flow`, and `flow->flow` (shared paths).  \n",
    "   - Normalize node features for GNN input (e.g., Min-Max or Standard scaling).  \n",
    "\n",
    "**Output:**  \n",
    "- Network topology (graph) with node and link attributes  \n",
    "- Routing paths for each flow  \n",
    "- Flow-level KPI metrics (delay, jitter, loss, delta, bandwidth, priority, user count, slice type)  \n",
    "- Heterogeneous graph (`pyg_graph`) ready for GNN input\n",
    "\n",
    "**Remarks:**  \n",
    "- Captures realistic network behavior including dynamic traffic, burstiness, and queueing effects.  \n",
    "- Output serves as the input for **Module 3: KPI Predictor (GNN)**.  \n",
    "- Supports multiple routing strategies and slice types for flexible simulation.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ouwmRuZOgF0S"
   },
   "outputs": [],
   "source": [
    "def generate_topology(num_nodes=20, seed=42, real_graph=None):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    G = nx.MultiDiGraph()\n",
    "    if real_graph:\n",
    "        G = real_graph.copy()\n",
    "    else:\n",
    "        base = nx.erdos_renyi_graph(n=num_nodes, p=0.15, seed=seed)\n",
    "        for u, v in base.edges():\n",
    "            for a, b in [(u, v), (v, u)]:\n",
    "                bw = random.choice([50, 100, 200, 500])  # Mbps\n",
    "                latency = round(random.uniform(0.5, 5.0), 3)  # ms\n",
    "                port = random.randint(0, 3)\n",
    "                buffer = random.randint(50, 500)  # packets\n",
    "                G.add_edge(a, b, bandwidth=bw, latency_base=latency, port=port, buffer=buffer)\n",
    "    return G\n",
    "\n",
    "def mm1_delay(mu, lam):\n",
    "    if lam >= mu: return float('inf')\n",
    "    return 1.0 / (mu - lam)\n",
    "\n",
    "def derive_traffic(G, intents, packet_size=1200, burst_factor=0.3, timesteps=5):\n",
    "    nodes = list(G.nodes())\n",
    "    T = defaultdict(lambda: {'Flows': []})\n",
    "    fid = 0\n",
    "\n",
    "    # Slice-specific parameters\n",
    "    slice_params = {\n",
    "        \"eMBB\": {\"buffer\":200, \"mu_factor\":1.0},\n",
    "        \"URLLC\": {\"buffer\":50, \"mu_factor\":1.2},\n",
    "        \"mIoT\": {\"buffer\":500, \"mu_factor\":0.8}\n",
    "    }\n",
    "\n",
    "    for intent in intents:\n",
    "        num_users = intent['user_count']\n",
    "        total_bw = intent['bandwidth']  # Mbps\n",
    "        flows_count = max(1, math.ceil(num_users / 100))\n",
    "        per_flow_bw = total_bw / flows_count\n",
    "\n",
    "        for _ in range(flows_count):\n",
    "            src, dst = random.sample(nodes, 2)\n",
    "            # Generate time-varying bandwidth per flow\n",
    "            bw_time = [per_flow_bw * (1 + burst_factor*random.uniform(-1,1)) for _ in range(timesteps)]\n",
    "            flow = {\n",
    "                \"flow_id\": fid,\n",
    "                \"origin_node\": src,\n",
    "                \"dest_node\": dst,\n",
    "                \"bandwidth_t\": bw_time,\n",
    "                \"avgPacketSize\": packet_size,\n",
    "                \"type\": intent['slice_type'],            # fixed here\n",
    "                \"priority\": intent['priority'],\n",
    "                \"target_latency\": intent['latency'],\n",
    "                \"user_count\": intent['user_count'],\n",
    "                \"buffer\": slice_params[intent['slice_type']]['buffer'],\n",
    "                \"mu_factor\": slice_params[intent['slice_type']]['mu_factor']  # fixed here\n",
    "            }\n",
    "            T[(src, dst)]['Flows'].append(flow)\n",
    "            fid += 1\n",
    "    return T\n",
    "\n",
    "def route_and_metrics(G, T, routing='shortest', packet_size=1200):\n",
    "    R, metrics = {}, []\n",
    "\n",
    "    for (src,dst), entry in T.items():\n",
    "        tempG = nx.DiGraph()\n",
    "        for u,v,d in G.edges(data=True):\n",
    "            tempG.add_edge(u, v, weight=d['latency_base'])\n",
    "        try:\n",
    "            if routing=='shortest':\n",
    "                path = nx.shortest_path(tempG, source=src, target=dst, weight='weight')\n",
    "            elif routing=='ecmp':\n",
    "                paths = list(nx.all_shortest_paths(tempG, source=src, target=dst, weight='weight'))\n",
    "                path = random.choice(paths)\n",
    "            else:\n",
    "                path = nx.shortest_path(tempG, source=src, target=dst, weight='weight')\n",
    "        except:\n",
    "            path = [src, dst]\n",
    "        R[(src,dst)] = path\n",
    "\n",
    "        for f in entry['Flows']:\n",
    "            delays_all, losses_all = [], []\n",
    "            for bw in f['bandwidth_t']:  # time-varying\n",
    "                lam = (bw*1e6/8)/packet_size\n",
    "                delays, losses = [], []\n",
    "                for u,v in zip(path[:-1], path[1:]):\n",
    "                    edata = G.get_edge_data(u,v)\n",
    "                    if not edata:\n",
    "                        edata = {'bandwidth':100, 'latency_base':1.0, 'buffer':100}\n",
    "                    else:\n",
    "                        edata = list(edata.values())[0]\n",
    "                    mu = (edata['bandwidth']*1e6/8)/packet_size * f['mu_factor']\n",
    "                    qd = mm1_delay(mu, lam)\n",
    "                    qd_ms = 1000 if math.isinf(qd) else qd*1000\n",
    "                    delays.append(edata['latency_base'] + qd_ms)\n",
    "                    loss = max(0.0,(lam/mu-1.0)*100.0) if mu>0 and lam>mu else 0.0\n",
    "                    losses.append(loss)\n",
    "                delays_all.append(sum(delays))\n",
    "                losses_all.append(max(losses))\n",
    "            path_delay = np.mean(delays_all)\n",
    "            jitter = np.std(delays_all)\n",
    "            loss_pct = np.mean(losses_all)\n",
    "            delta = f['target_latency'] - path_delay\n",
    "            metrics.append({\n",
    "                \"flow_id\": f['flow_id'], \"src\": src, \"dst\": dst, \"path\": path,\n",
    "                \"delay\": path_delay, \"jitter\": jitter, \"loss\": loss_pct,\n",
    "                \"bw\": np.mean(f['bandwidth_t']), \"delta\": delta, \"priority\": f['priority'],\n",
    "                \"user_count\": f['user_count'], \"slice_type\": f['type']\n",
    "            })\n",
    "    return R, metrics\n",
    "\n",
    "def build_pyg_graph(G, metrics):\n",
    "    \"\"\"\n",
    "    Creates heterogeneous graph with nodes: link, queue, flow\n",
    "    Edges: link->flow, queue->flow, flow->flow dependencies\n",
    "    \"\"\"\n",
    "    data = HeteroData()\n",
    "    pr_map = {'low':0, 'medium':1, 'high':2}\n",
    "\n",
    "    link_feats = [[d['bandwidth'], d['latency_base'], d.get('buffer',100)] for _,_,d in G.edges(data=True)]\n",
    "    if link_feats:\n",
    "        link_feats = np.array(link_feats, dtype=float)\n",
    "        data['link'].x = torch.tensor(MinMaxScaler().fit_transform(link_feats), dtype=torch.float32)\n",
    "\n",
    "    queue_feats = [[d.get('buffer',100)] for _,_,d in G.edges(data=True)]\n",
    "    if queue_feats:\n",
    "        queue_feats = np.array(queue_feats, dtype=float)\n",
    "        data['queue'].x = torch.tensor(MinMaxScaler().fit_transform(queue_feats), dtype=torch.float32)\n",
    "\n",
    "    path_feats = []\n",
    "    for m in metrics:\n",
    "        path_feats.append([\n",
    "            m['bw'], m['delay'], m['jitter'], m['loss'], m['delta'],\n",
    "            pr_map.get(m['priority'],1), m['user_count']\n",
    "        ])\n",
    "    if path_feats:\n",
    "        path_feats = np.array(path_feats, dtype=float)\n",
    "        path_feats[:,4] = StandardScaler().fit_transform(path_feats[:,4].reshape(-1,1)).flatten()\n",
    "        data['flow'].x = torch.tensor(MinMaxScaler().fit_transform(path_feats), dtype=torch.float32)\n",
    "\n",
    "    src_idx, dst_idx = [], []\n",
    "    link_list = list(G.edges(keys=True))\n",
    "    for i, m in enumerate(metrics):\n",
    "        for u,v in zip(m['path'][:-1], m['path'][1:]):\n",
    "            link_idx = next((idx for idx,(a,b,k) in enumerate(link_list) if a==u and b==v), None)\n",
    "            if link_idx is not None:\n",
    "                src_idx.append(link_idx)\n",
    "                dst_idx.append(i)\n",
    "    if src_idx:\n",
    "        data['link','to','flow'].edge_index = torch.tensor([src_idx, dst_idx], dtype=torch.long)\n",
    "\n",
    "    src_idx, dst_idx = [], []\n",
    "    for i, m in enumerate(metrics):\n",
    "        for u,v in zip(m['path'][:-1], m['path'][1:]):\n",
    "            queue_idx = next((idx for idx,(a,b,k) in enumerate(link_list) if a==u and b==v), None)\n",
    "            if queue_idx is not None:\n",
    "                src_idx.append(queue_idx)\n",
    "                dst_idx.append(i)\n",
    "    if src_idx:\n",
    "        data['queue','to','flow'].edge_index = torch.tensor([src_idx, dst_idx], dtype=torch.long)\n",
    "\n",
    "    src_idx, dst_idx = [], []\n",
    "    for i, fi in enumerate(metrics):\n",
    "        for j, fj in enumerate(metrics):\n",
    "            if i!=j and len(set(fi['path']).intersection(set(fj['path'])))>0:\n",
    "                src_idx.append(i)\n",
    "                dst_idx.append(j)\n",
    "    if src_idx:\n",
    "        data['flow','to','flow'].edge_index = torch.tensor([src_idx,dst_idx], dtype=torch.long)\n",
    "\n",
    "    return data\n",
    "\n",
    "def module2(intents, num_nodes=20, seed=42, routing='shortest'):\n",
    "    topo = generate_topology(num_nodes=num_nodes, seed=seed)\n",
    "    T = derive_traffic(topo, intents, timesteps=5)\n",
    "    R, metrics = route_and_metrics(topo, T, routing=routing)\n",
    "    g = build_pyg_graph(topo, metrics)\n",
    "    return {\"topology\": topo, \"routing\": R, \"metrics\": metrics, \"pyg_graph\": g}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5haD2qLUAvDk"
   },
   "source": [
    "## Module 3: KPI Predictor (Graph Neural Network)\n",
    "\n",
    "**Purpose:**  \n",
    "Predict slice-level Key Performance Indicators (KPIs) — delay, jitter, and packet loss — using a lightweight graph neural network (GNN) applied to the heterogeneous network graph generated in Module 2. This enables the framework to anticipate slice performance before admission or resource allocation decisions.\n",
    "\n",
    "**Input:**  \n",
    "- Heterogeneous graph (`pyg_graph`) from Module 2 containing:  \n",
    "  - Nodes: `flow`, `link`, `queue` (optional)  \n",
    "  - Edges representing dependencies: `link->flow`, `queue->flow`, `flow->flow`  \n",
    "- Flow-level metrics (targets) for supervised training  \n",
    "- Intent-based features (priority, delta, user count)  \n",
    "\n",
    "**Process:**  \n",
    "1. **Feature Preprocessing:**  \n",
    "   - Normalize node features (Min-Max or standard scaling).  \n",
    "   - Concatenate intent-based features to flow nodes.  \n",
    "\n",
    "2. **GNN Architecture:**  \n",
    "   - Use a **heterogeneous GAT (Graph Attention Network)** for message passing between nodes.  \n",
    "   - Node types (`flow`, `link`, `queue`) are projected into a hidden dimension.  \n",
    "   - Aggregates information along graph edges using attention mechanism.  \n",
    "\n",
    "3. **KPI Prediction:**  \n",
    "   - Apply a small feedforward network to predict three KPIs per flow: delay, jitter, and loss.  \n",
    "   - Train the model using regression loss (e.g., MSE) and evaluate using metrics such as SMAPE.  \n",
    "\n",
    "4. **Explainability:**  \n",
    "   - Use Integrated Gradients or similar XAI techniques to attribute predicted KPIs to input features.  \n",
    "   - Identify which node or feature contributes most to high delay, jitter, or loss.  \n",
    "\n",
    "**Output:**  \n",
    "- Predicted KPIs for each flow in the network graph:  \n",
    "  ```json\n",
    "  [\n",
    "    {\"flow_id\": 0, \"delay\": 12.5, \"jitter\": 1.2, \"loss\": 0.3},\n",
    "    {\"flow_id\": 1, \"delay\": 8.7, \"jitter\": 0.9, \"loss\": 0.1},\n",
    "    ...\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04_ZDNpl_NOA"
   },
   "outputs": [],
   "source": [
    "class KPI_GNN(nn.Module):\n",
    "    def __init__(self, data, hidden_dim=32, intent_features_dim=2):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        in_channels_flow = data['flow'].x.shape[1] + intent_features_dim\n",
    "        self.flow_lin = nn.Linear(in_channels_flow, hidden_dim)\n",
    "\n",
    "        in_channels_link = data['link'].x.shape[1]\n",
    "        self.link_lin = nn.Linear(in_channels_link, hidden_dim)\n",
    "\n",
    "        in_channels_queue = data['queue'].x.shape[1] if 'queue' in data else 0\n",
    "        if in_channels_queue > 0:\n",
    "            self.queue_lin = nn.Linear(in_channels_queue, hidden_dim)\n",
    "\n",
    "        conv_dict = {\n",
    "            ('link','to','flow'): GATConv(hidden_dim, hidden_dim, heads=2, concat=False, add_self_loops=False),\n",
    "            ('flow','to','flow'): GATConv(hidden_dim, hidden_dim, heads=2, concat=False, add_self_loops=True)\n",
    "        }\n",
    "        if 'queue' in data:\n",
    "            conv_dict[('queue','to','flow')] = GATConv(hidden_dim, hidden_dim, heads=2, concat=False, add_self_loops=False)\n",
    "\n",
    "        self.conv = HeteroConv(conv_dict, aggr='mean')\n",
    "\n",
    "        self.flow_predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim//2, 3)  # Predict 3 KPIs: delay, jitter, loss\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        flow_x = (data['flow'].x - data['flow'].x.mean(0)) / (data['flow'].x.std(0) + 1e-6)\n",
    "        if hasattr(data['flow'], 'intent_features'):\n",
    "            flow_x = torch.cat([flow_x, data['flow'].intent_features], dim=1)\n",
    "        else:\n",
    "            flow_x = torch.cat([flow_x, torch.zeros(flow_x.shape[0], 2, device=flow_x.device)], dim=1)\n",
    "        flow_x = self.flow_lin(flow_x)\n",
    "\n",
    "        link_x = (data['link'].x - data['link'].x.mean(0)) / (data['link'].x.std(0) + 1e-6)\n",
    "        link_x = self.link_lin(link_x)\n",
    "\n",
    "        if 'queue' in data:\n",
    "            queue_x = (data['queue'].x - data['queue'].x.mean(0)) / (data['queue'].x.std(0) + 1e-6)\n",
    "            queue_x = self.queue_lin(queue_x)\n",
    "        else:\n",
    "            queue_x = None\n",
    "\n",
    "        x_dict = {'flow': flow_x, 'link': link_x}\n",
    "        if queue_x is not None:\n",
    "            x_dict['queue'] = queue_x\n",
    "\n",
    "        x_dict = self.conv(x_dict, data.edge_index_dict)\n",
    "        flow_out = F.relu(x_dict['flow'])\n",
    "        return self.flow_predictor(flow_out)\n",
    "\n",
    "def smape(y_true, y_pred, eps=1e-6):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return 100 / len(y_true) * np.sum(\n",
    "        2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + eps)\n",
    "    )\n",
    "\n",
    "def set_flow_targets(data, metrics):\n",
    "    y = torch.tensor([[m['delay'], m['jitter'], m['loss']] for m in metrics], dtype=torch.float32)\n",
    "    data['flow'].y = y\n",
    "    return data\n",
    "\n",
    "def train_model(model, data, epochs=50, lr=0.01, device=None):\n",
    "    device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    data = data.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    y_true = data['flow'].y\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(data)\n",
    "        loss = F.mse_loss(y_pred, y_true)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            smape_val = smape(y_true.detach().cpu().numpy(), y_pred.detach().cpu().numpy())\n",
    "            print(f\"Epoch {epoch}: Loss={loss.item():.4f}, SMAPE={smape_val:.2f}%\")\n",
    "    return model\n",
    "\n",
    "def explain_predictions(model, data, node_type='flow', feature_idx=0):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    flow_x = (data['flow'].x - data['flow'].x.mean(0)) / (data['flow'].x.std(0) + 1e-6)\n",
    "    flow_x = flow_x.to(device)\n",
    "    if hasattr(data['flow'], 'intent_features'):\n",
    "        flow_x = torch.cat([flow_x, data['flow'].intent_features.to(device)], dim=1)\n",
    "    flow_x.requires_grad = True\n",
    "\n",
    "    link_x = (data['link'].x - data['link'].x.mean(0)) / (data['link'].x.std(0) + 1e-6)\n",
    "    link_x = model.link_lin(link_x.to(device))\n",
    "    if 'queue' in data:\n",
    "        queue_x = (data['queue'].x - data['queue'].x.mean(0)) / (data['queue'].x.std(0)+1e-6)\n",
    "        queue_x = model.queue_lin(queue_x.to(device))\n",
    "    else:\n",
    "        queue_x = torch.zeros((0, model.flow_lin.out_features), device=device)\n",
    "\n",
    "    def forward_wrapper(flow_feats):\n",
    "        x_dict = {\n",
    "            'flow': model.flow_lin(flow_feats),\n",
    "            'link': link_x,\n",
    "            'queue': queue_x\n",
    "        }\n",
    "        x_out = model.conv(x_dict, data.edge_index_dict)\n",
    "        return model.flow_predictor(F.relu(x_out['flow']))[:, feature_idx]\n",
    "\n",
    "    from captum.attr import IntegratedGradients\n",
    "    ig = IntegratedGradients(forward_wrapper)\n",
    "    return ig.attribute(flow_x).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_S6qK1k3A3EG"
   },
   "source": [
    "## Module 4: Slice Admission & Resource Allocation (RL-Based)\n",
    "\n",
    "**Purpose:**  \n",
    "Decide in real-time whether to admit a network slice and how to allocate resources (e.g., bandwidth) using reinforcement learning. This module ensures SLA compliance while optimizing network utilization and fairness.\n",
    "\n",
    "**Input:**  \n",
    "- Predicted KPIs from Module 3 (delay, jitter, loss per flow)  \n",
    "- Structured intent JSON from Module 1 (slice type, latency, jitter, bandwidth, priority, user count)  \n",
    "- Environment constraints such as maximum available bandwidth  \n",
    "\n",
    "**Process:**  \n",
    "1. **Environment Setup:**  \n",
    "   - Model the network slicing scenario as a Gymnasium environment.  \n",
    "   - Each step represents processing a single slice request.  \n",
    "   - Observation includes normalized metrics, slice intent features, and graph-level statistics.\n",
    "\n",
    "2. **Action Space:**  \n",
    "   - Admit or reject the slice.  \n",
    "   - Allocate bandwidth (up to maximum available).  \n",
    "\n",
    "3. **Reward Design:**  \n",
    "   - Positive reward for meeting slice latency, jitter, and loss targets.  \n",
    "   - Negative reward for SLA violations, over-allocation, or rejection of feasible slices.  \n",
    "   - Encourage fairness and efficient resource usage across slices.\n",
    "\n",
    "4. **Reinforcement Learning Agent:**  \n",
    "   - Use policy-gradient or actor-critic algorithms (e.g., PPO) to learn optimal admission and allocation policies.  \n",
    "   - Train the agent over multiple episodes to generalize across varying slice requests and network conditions.  \n",
    "\n",
    "5. **State Update:**  \n",
    "   - After each action, environment updates to the next slice request.  \n",
    "   - Track cumulative rewards, terminated state, and ongoing resource usage.\n",
    "\n",
    "**Output:**  \n",
    "- Slice admission decision (`admit`: true/false)  \n",
    "- Bandwidth allocation for each slice  \n",
    "- Updated network state after each action  \n",
    "- Learned policy that can generalize to new slice requests  \n",
    "\n",
    "**Remarks:**  \n",
    "- Enables adaptive and intent-aware slice admission and resource allocation.  \n",
    "- Works in conjunction with KPI predictions from Module 3 to proactively avoid SLA violations.  \n",
    "- Provides feedback for **Module 5: Slice Lifecycle Management** to monitor and adjust active slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4Z6_t5XA24O"
   },
   "outputs": [],
   "source": [
    "class SliceEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, intents, metrics, max_bandwidth=100):\n",
    "        super().__init__()\n",
    "        self.intents = intents\n",
    "        self.metrics = metrics\n",
    "        self.max_bandwidth = max_bandwidth\n",
    "        self.current_slice = 0\n",
    "        self.obs_dim = 8\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(self.obs_dim,), dtype=np.float32)\n",
    "        self.action_space = gym.spaces.MultiDiscrete([2, max_bandwidth+1])\n",
    "        self.state = self._get_state()\n",
    "        self.done = False\n",
    "\n",
    "    def _get_state(self):\n",
    "        if self.current_slice >= len(self.intents):\n",
    "            return np.zeros(self.obs_dim, dtype=np.float32)\n",
    "        intent = self.intents[self.current_slice]\n",
    "        metric = self.metrics[self.current_slice]\n",
    "        delay = min(metric['delay']/intent['latency'], 1.0)\n",
    "        jitter = min(metric['jitter']/intent['jitter'], 1.0)\n",
    "        loss = min(metric['loss']/0.1, 1.0)\n",
    "        user_count = min(intent['user_count']/1000.0, 1.0)\n",
    "        priority = {'low':0.0, 'medium':0.5, 'high':1.0}.get(intent['priority'],0.5)\n",
    "        graph_feats = [\n",
    "            metric['total_link_bw'],\n",
    "            metric['avg_flow_delay'],\n",
    "            metric['avg_flow_jitter']\n",
    "        ]\n",
    "        return np.array([delay, jitter, loss, user_count, priority] + graph_feats, dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        admit, bw_alloc = action\n",
    "        intent = self.intents[self.current_slice]\n",
    "        metric = self.metrics[self.current_slice]\n",
    "        reward = 0.0\n",
    "        if admit:\n",
    "            reward += max(0, intent['latency'] - metric['delay'])\n",
    "            reward += max(0, intent['jitter'] - metric['jitter'])\n",
    "            reward += max(0, 0.1 - metric['loss'])\n",
    "            reward -= max(0, bw_alloc - intent['bandwidth'])*0.01\n",
    "        else:\n",
    "            reward -= 0.1\n",
    "        self.current_slice += 1\n",
    "        terminated = self.current_slice >= len(self.intents)\n",
    "        truncated = False\n",
    "        self.state = self._get_state()\n",
    "        return self.state, reward, terminated, truncated, {}\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.current_slice = 0\n",
    "        self.done = False\n",
    "        self.state = self._get_state()\n",
    "        return self.state, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f\"Slice {self.current_slice}/{len(self.intents)} - State: {self.state}\")\n",
    "\n",
    "def create_rl_agent(env, total_timesteps=5000):\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=0)\n",
    "    model.learn(total_timesteps=total_timesteps)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dg9rHSVhHmf6"
   },
   "source": [
    "## Module 5: Slice Lifecycle Manager\n",
    "\n",
    "**Purpose:**  \n",
    "Monitor active network slices, ensure SLA compliance, and trigger dynamic scaling or teardown actions based on KPI updates. Maintains slice status throughout its lifecycle.\n",
    "\n",
    "**Input:**  \n",
    "- RL agent outputs from Module 4 (admit/reject decisions, bandwidth allocations)  \n",
    "- Real-time or simulated KPI updates per slice (delay, jitter, loss)  \n",
    "- SLA thresholds for each KPI (optional; defaults: delay=0.05, jitter=0.01, loss=0.01)\n",
    "\n",
    "**Process:**  \n",
    "1. **Initialization:**  \n",
    "   - Track each slice’s status (active/inactive), SLA violations, and rescaling triggers.  \n",
    "   - Set default thresholds for delay, jitter, and loss if not provided.\n",
    "\n",
    "2. **Metrics Update:**  \n",
    "   - Receive periodic KPI updates for each slice.  \n",
    "   - Compare each KPI against SLA thresholds.  \n",
    "   - Count the number of SLA violations per slice.  \n",
    "   - Trigger rescaling if any KPI exceeds its threshold; otherwise, mark the slice as compliant.\n",
    "\n",
    "3. **Status Tracking:**  \n",
    "   - Maintain a structured record of all slices including:  \n",
    "     - `slice_id`  \n",
    "     - Admission status (`admit`)  \n",
    "     - Current operational status (`active` or `inactive`)  \n",
    "     - Number of SLA violations  \n",
    "     - Rescaling triggers  \n",
    "     - Current KPI metrics  \n",
    "\n",
    "4. **Simulation (Optional):**  \n",
    "   - Simulate time-varying KPI updates by adding small random noise to predicted KPIs.  \n",
    "   - Iterate over multiple time steps to emulate realistic dynamic network conditions.  \n",
    "\n",
    "**Output:**  \n",
    "- Updated slice status for all slices:  \n",
    "  ```json\n",
    "  [\n",
    "    {\n",
    "      \"slice_id\": 0,\n",
    "      \"admit\": true,\n",
    "      \"status\": \"active\",\n",
    "      \"sla_violations\": 0,\n",
    "      \"rescale_triggered\": false,\n",
    "      \"current_metrics\": {\"delay\": 0.012, \"jitter\": 0.003, \"loss\": 0.001}\n",
    "    },\n",
    "    ...\n",
    "  ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sO3u5UPTA21r"
   },
   "outputs": [],
   "source": [
    "class SliceLifecycleManager:\n",
    "    def __init__(self, rl_output, sla_thresholds=None):\n",
    "        self.slices = rl_output\n",
    "        self.num_slices = len(rl_output)\n",
    "        self.sla_thresholds = sla_thresholds or {\n",
    "            \"delay\": 0.05,\n",
    "            \"jitter\": 0.01,\n",
    "            \"loss\": 0.01\n",
    "        }\n",
    "        self.status = [\n",
    "            {\n",
    "                \"slice_id\": s['slice_id'],\n",
    "                \"admit\": s['admit'],\n",
    "                \"status\": \"active\" if s['admit'] else \"inactive\",\n",
    "                \"sla_violations\": 0,\n",
    "                \"rescale_triggered\": False,\n",
    "                \"current_metrics\": {\"delay\": 0.0, \"jitter\": 0.0, \"loss\": 0.0}\n",
    "            } for s in rl_output\n",
    "        ]\n",
    "\n",
    "    def update_slice_metrics(self, kpi_updates):\n",
    "        for i, kpi in enumerate(kpi_updates):\n",
    "            if not self.slices[i]['admit']:\n",
    "                continue\n",
    "            self.status[i]['current_metrics'] = kpi\n",
    "            violations = 0\n",
    "            for k in [\"delay\", \"jitter\", \"loss\"]:\n",
    "                if kpi[k] > self.sla_thresholds[k]:\n",
    "                    violations += 1\n",
    "            self.status[i]['sla_violations'] = violations\n",
    "            if violations > 0:\n",
    "                self.status[i]['rescale_triggered'] = True\n",
    "            else:\n",
    "                self.status[i]['rescale_triggered'] = False\n",
    "\n",
    "    def get_status(self):\n",
    "        return self.status\n",
    "\n",
    "def simulate_kpi_updates(rl_output, predicted_kpis, steps=10, noise_scale=0.002):\n",
    "    num_slices = len(rl_output)\n",
    "\n",
    "    for t in range(steps):\n",
    "        kpi_updates = []\n",
    "        for i in range(num_slices):\n",
    "            if not rl_output[i]['admit']:\n",
    "                kpi_updates.append({\"delay\": 0.0, \"jitter\": 0.0, \"loss\": 0.0})\n",
    "                continue\n",
    "            delay = max(0, predicted_kpis['delay'][i] + np.random.normal(0, noise_scale))\n",
    "            jitter = max(0, predicted_kpis['jitter'][i] + np.random.normal(0, noise_scale))\n",
    "            loss = max(0, predicted_kpis['loss'][i] + np.random.normal(0, noise_scale))\n",
    "            kpi_updates.append({\"delay\": delay, \"jitter\": jitter, \"loss\": loss})\n",
    "\n",
    "        yield kpi_updates\n",
    "        time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtvrEpVlKgeN"
   },
   "source": [
    "## Module 6: Explainability Layer & Dashboard\n",
    "\n",
    "**Purpose:**  \n",
    "Provide interpretability and transparency for the predictions and decisions made by the GNN-based KPI predictor (Module 3) and the RL-based slice admission agent (Module 4). Generate a consolidated dashboard summarizing explanations and key evaluation metrics for network slices.\n",
    "\n",
    "**Input:**  \n",
    "- Trained GNN model from Module 3  \n",
    "- Trained RL agent from Module 4  \n",
    "- Heterogeneous graph (`pyg_graph`) containing network, flow, and queue features  \n",
    "- RL outputs including slice admission and bandwidth allocation  \n",
    "- Predicted KPIs (delay, jitter, loss)  \n",
    "- Optional: Flow feature names for readable explanations  \n",
    "\n",
    "**Process:**  \n",
    "1. **GNN Explainability:**  \n",
    "   - Apply Integrated Gradients to the flow node features of the GNN.  \n",
    "   - Identify which features most influence predicted KPIs (delay, jitter, loss).  \n",
    "   - Return per-flow feature attributions.  \n",
    "\n",
    "2. **RL Explainability:**  \n",
    "   - Use SHAP (KernelExplainer) to attribute slice admission and allocation decisions to input features.  \n",
    "   - Determine which state variables most influenced the agent’s decisions.  \n",
    "\n",
    "3. **Evaluation Metrics Computation:**  \n",
    "   - SLA satisfaction ratio: fraction of admitted slices meeting KPI thresholds.  \n",
    "   - Bandwidth utilization: fraction of allocated bandwidth over maximum available.  \n",
    "   - Fairness index (Jain’s index) for bandwidth allocation across slices.  \n",
    "   - Admission rate: fraction of slices admitted.  \n",
    "   - Policy efficiency: combined measure of SLA satisfaction, utilization, and fairness.  \n",
    "\n",
    "4. **Dashboard Generation:**  \n",
    "   - Summarize GNN feature importances per flow.  \n",
    "   - Summarize RL decision explanations per action.  \n",
    "   - Include computed evaluation metrics for overall network slicing performance.  \n",
    "\n",
    "**Output:**  \n",
    "- GNN explanations per flow, highlighting most influential features  \n",
    "- RL decision explanations per slice, highlighting key influencing state features  \n",
    "- Performance and fairness metrics:  \n",
    "  ```json\n",
    "  {\n",
    "    \"gnn_explanations\": [\"Flow 0 influenced by delay(0.12), bw(0.08), priority(0.05)\", \"...\"],\n",
    "    \"rl_explanations\": [\"Decision 0 influenced by feature_2(0.15), feature_4(0.10), feature_0(0.08)\", \"...\"],\n",
    "    \"evaluation_metrics\": {\n",
    "      \"sla_satisfaction_ratio\": 0.9,\n",
    "      \"bandwidth_utilization\": 0.85,\n",
    "      \"fairness_index\": 0.95,\n",
    "      \"admission_rate\": 0.8,\n",
    "      \"policy_efficiency\": 0.73\n",
    "    }\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NnRi1g0IA2wT"
   },
   "outputs": [],
   "source": [
    "class Module6Dashboard:\n",
    "    def __init__(self, gnn_model, rl_agent, pyg_graph, rl_output, predicted_kpis, flow_feature_names=None):\n",
    "        self.gnn_model = gnn_model\n",
    "        self.rl_agent = rl_agent\n",
    "        self.data = pyg_graph\n",
    "        self.rl_output = rl_output\n",
    "        self.predicted_kpis = predicted_kpis\n",
    "        self.flow_feature_names = flow_feature_names or [\"bw\", \"delay\", \"jitter\", \"loss\", \"delta\", \"priority\", \"user_count\"]\n",
    "\n",
    "    def gnn_explain(self, feature_idx=0):\n",
    "        self.gnn_model.eval()\n",
    "        device = next(self.gnn_model.parameters()).device\n",
    "\n",
    "        flow_x = self.data['flow'].x.to(device)\n",
    "        if flow_x.shape[1] != self.gnn_model.flow_lin.in_features:\n",
    "            raise ValueError(f\"flow_x has {flow_x.shape[1]} features, but model expects {self.gnn_model.flow_lin.in_features}\")\n",
    "        flow_x.requires_grad = True\n",
    "\n",
    "\n",
    "        def forward_wrapper(x):\n",
    "            x_dict = {\n",
    "                'flow': self.gnn_model.flow_lin(x),   # x must have same dim as trained GNN\n",
    "                'link': self.gnn_model.link_lin(self.data['link'].x.to(device))\n",
    "            }\n",
    "            if 'queue' in self.data:\n",
    "                x_dict['queue'] = self.gnn_model.queue_lin(self.data['queue'].x.to(device))\n",
    "            x_out = self.gnn_model.conv(x_dict, self.data.edge_index_dict)\n",
    "            return self.gnn_model.flow_predictor(torch.relu(x_out['flow']))[:, feature_idx]\n",
    "\n",
    "        ig = IntegratedGradients(forward_wrapper)\n",
    "        attr = ig.attribute(flow_x)\n",
    "        return attr.detach().cpu().numpy()\n",
    "\n",
    "    def rl_explain(self, state_samples, nsamples=50):\n",
    "        def rl_forward(states):\n",
    "            actions = []\n",
    "            for s in states:\n",
    "                s_tensor = np.array(s).reshape(1,-1)\n",
    "                a, _ = self.rl_agent.predict(s_tensor, deterministic=True)\n",
    "                actions.append(a[0])\n",
    "            return np.array(actions)\n",
    "\n",
    "        explainer = shap.KernelExplainer(rl_forward, np.array(state_samples[:10]))\n",
    "        shap_values = explainer.shap_values(np.array(state_samples[:nsamples]))\n",
    "        return shap_values\n",
    "\n",
    "    def compute_metrics(self):\n",
    "        metrics = {}\n",
    "        sla_ok = []\n",
    "        for i, s in enumerate(self.rl_output):\n",
    "            if not s['admit']:\n",
    "                sla_ok.append(False)\n",
    "                continue\n",
    "            kpi = self.predicted_kpis\n",
    "            delay_ok = kpi['delay'][i] <= s.get('latency', 50)\n",
    "            jitter_ok = kpi['jitter'][i] <= s.get('jitter', 5)\n",
    "            loss_ok = kpi['loss'][i] <= 0.05\n",
    "            sla_ok.append(delay_ok and jitter_ok and loss_ok)\n",
    "        metrics['sla_satisfaction_ratio'] = sum(sla_ok)/len(sla_ok)\n",
    "\n",
    "        total_alloc = sum([s['bandwidth_allocation'] if s['admit'] else 0 for s in self.rl_output])\n",
    "        max_possible = sum([s.get('bandwidth',100) for s in self.rl_output])\n",
    "        metrics['bandwidth_utilization'] = total_alloc/max_possible if max_possible>0 else 0.0\n",
    "\n",
    "        allocs = np.array([s['bandwidth_allocation'] if s['admit'] else 0 for s in self.rl_output])\n",
    "        if np.sum(allocs) > 0:\n",
    "            metrics['fairness_index'] = np.sum(allocs)**2 / (len(allocs) * np.sum(allocs**2))\n",
    "        else:\n",
    "            metrics['fairness_index'] = 0.0\n",
    "\n",
    "        metrics['admission_rate'] = sum([s['admit'] for s in self.rl_output])/len(self.rl_output)\n",
    "        metrics['policy_efficiency'] = metrics['sla_satisfaction_ratio'] * metrics['bandwidth_utilization'] * metrics['fairness_index']\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def generate_dashboard(self, gnn_attr=None, rl_shap=None, top_features=3):\n",
    "        gnn_msgs = []\n",
    "        if gnn_attr is not None:\n",
    "            for i, attr in enumerate(gnn_attr):\n",
    "                top_idx = np.argsort(-np.abs(attr))[:top_features]\n",
    "                features = [self.flow_feature_names[j] for j in top_idx]\n",
    "                values = [attr[j] for j in top_idx]\n",
    "                gnn_msgs.append(f\"Flow {i} influenced by \" + \", \".join([f\"{f}({v:.3f})\" for f,v in zip(features, values)]))\n",
    "\n",
    "        rl_msgs = []\n",
    "        if rl_shap is not None:\n",
    "            for i, shap_vals in enumerate(rl_shap):\n",
    "                top_idx = np.argsort(-np.abs(shap_vals))[:top_features]\n",
    "                features = [f\"feature_{j}\" for j in top_idx]\n",
    "                values = [shap_vals[j] for j in top_idx]\n",
    "                rl_msgs.append(\n",
    "                    f\"Decision {i} influenced by \" +\n",
    "                    \", \".join([f\"{f}({np.mean(v):.3f})\" if isinstance(v, np.ndarray) else f\"{f}({v:.3f})\"\n",
    "                              for f, v in zip(features, values)])\n",
    "                )\n",
    "        metrics = self.compute_metrics()\n",
    "\n",
    "        dashboard = {\n",
    "            \"gnn_explanations\": gnn_msgs,\n",
    "            \"rl_explanations\": rl_msgs,\n",
    "            \"evaluation_metrics\": metrics\n",
    "        }\n",
    "        return dashboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYk-fDubniyB"
   },
   "source": [
    "## Run All Modules: End-to-End Simulation\n",
    "\n",
    "**Purpose:**  \n",
    "Execute the full workflow of the Lightweight Intent-Aware Network Slicing framework, from intent parsing to KPI prediction, RL-based slice admission, lifecycle management, and explainability dashboard generation. This demonstrates the integration of all modules and provides actionable insights on slice performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "26f4c925eafc4c1cb3e56b7b221b3aed",
      "0d2921c6e91f4e09a7116fc425710c44",
      "6442c48c69e047b3a71ef5632ec2d574",
      "e828bab4a30143ed80497a5e7ee593b9",
      "5721c710701c4465a753592c560d1301",
      "579873af3e454b24b58b4bab80d36ee1",
      "75c9cec2f1204d6ebf9a045d526eb25d",
      "9679452afdad4a03a7f399f5330f9eac",
      "b76bab8539a94d6997714d787b982a69",
      "7e0460f89e694d95b1675ac89f81e2c7",
      "6e8947fe373e4b3d8043eb4d16e3e091"
     ]
    },
    "id": "bdhJrlzVA2sL",
    "outputId": "e7126cf0-a4b0-4af3-ba5a-b7ed43927626"
   },
   "outputs": [],
   "source": [
    "# Module 1:\n",
    "example_request = \"low-latency video streaming for 1000 users with 10ms delay and 5ms jitter\"\n",
    "intent_output = parse_intent_llm(example_request)\n",
    "print(\"Intent parsed:\", intent_output)\n",
    "\n",
    "# Module 2:\n",
    "result = module2([intent_output], num_nodes=20)\n",
    "data = result['pyg_graph']\n",
    "metrics = result['metrics']\n",
    "num_links, num_flows = data['link'].x.shape[0], data['flow'].x.shape[0]\n",
    "print(\"Graph info:\", data)\n",
    "print(\"Sample metrics:\", metrics[:2])\n",
    "\n",
    "slice_map = {\"low\": 0, \"medium\": 1, \"high\": 2}\n",
    "intent_feats = np.array([[slice_map[m['priority']], m['user_count']/1000.0] for m in metrics])\n",
    "data['flow'].intent_features = torch.tensor(intent_feats, dtype=torch.float32)\n",
    "\n",
    "kpi_labels = np.array([[m['delay'], m['jitter'], m['loss']] for m in metrics])\n",
    "data['flow'].y = torch.tensor(kpi_labels, dtype=torch.float32)\n",
    "\n",
    "# Module 3:\n",
    "hidden_dim = 32\n",
    "model = KPI_GNN(data, hidden_dim=hidden_dim)\n",
    "model = train_model(model, data, epochs=50, lr=0.01)\n",
    "model.eval()\n",
    "flow_preds = model(data).detach().numpy()\n",
    "predicted_kpis = {\n",
    "    \"delay\": flow_preds[:,0].tolist(),\n",
    "    \"jitter\": flow_preds[:,1].tolist(),\n",
    "    \"loss\": flow_preds[:,2].tolist()\n",
    "}\n",
    "print(\"Predicted KPIs:\", predicted_kpis)\n",
    "attributions = explain_predictions(model, data, feature_idx=0)\n",
    "print(\"Feature attributions (delay):\", attributions)\n",
    "\n",
    "# Module 4:\n",
    "num_flows = data['flow'].x.shape[0]\n",
    "total_link_bw = data['link'].x[:,0].sum().item()\n",
    "avg_flow_delay = data['flow'].x[:,1].mean().item()\n",
    "avg_flow_jitter = data['flow'].x[:,2].mean().item()\n",
    "rl_metrics = []\n",
    "for i, m in enumerate(metrics):\n",
    "    rl_metrics.append({\n",
    "        \"delay\": predicted_kpis['delay'][i],\n",
    "        \"jitter\": predicted_kpis['jitter'][i],\n",
    "        \"loss\": predicted_kpis['loss'][i],\n",
    "        \"bandwidth\": m['bw'],\n",
    "        \"priority\": m['priority'],\n",
    "        \"latency\": m['delta'] + m['delay'],\n",
    "        \"user_count\": m['user_count'],\n",
    "        \"total_link_bw\": total_link_bw,\n",
    "        \"avg_flow_delay\": avg_flow_delay,\n",
    "        \"avg_flow_jitter\": avg_flow_jitter\n",
    "    })\n",
    "env = SliceEnv([intent_output]*num_flows, rl_metrics)\n",
    "model_rl = create_rl_agent(env, total_timesteps=5000)\n",
    "\n",
    "state, _ = env.reset()\n",
    "actions_taken = []\n",
    "for i in range(num_flows):\n",
    "    action, _ = model_rl.predict(state)\n",
    "    actions_taken.append(action)\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    if terminated:\n",
    "        break\n",
    "\n",
    "output = []\n",
    "for i, a in enumerate(actions_taken):\n",
    "    slice_out = {\n",
    "        \"slice_id\": i,\n",
    "        \"admit\": bool(a[0]),\n",
    "        \"bandwidth_allocation\": int(a[1]),\n",
    "        \"routing_update\": []\n",
    "    }\n",
    "    output.append(slice_out)\n",
    "print(\"RL Agent Decisions:\")\n",
    "for o in output:\n",
    "    print(o)\n",
    "\n",
    "# Module 5:\n",
    "manager = SliceLifecycleManager(rl_output=output, sla_thresholds={\n",
    "    \"delay\": 0.01,\n",
    "    \"jitter\": 0.005,\n",
    "    \"loss\": 0.01\n",
    "})\n",
    "for kpi_step in simulate_kpi_updates(output, predicted_kpis, steps=20, noise_scale=0.002):\n",
    "    manager.update_slice_metrics(kpi_step)\n",
    "    current_status = manager.get_status()\n",
    "    print(\"Slice Status Update:\")\n",
    "    for s in current_status:\n",
    "        print(s)\n",
    "    for s in current_status:\n",
    "        if s['rescale_triggered']:\n",
    "            print(f\"Slice {s['slice_id']} requires rescaling due to SLA violations.\")\n",
    "\n",
    "# Module 6:\n",
    "flow_feature_names = [\"bw\", \"delay\", \"jitter\", \"loss\", \"delta\", \"priority\", \"user_count\", \"feat8\", \"feat9\"]\n",
    "data['flow'].x = torch.cat([data['flow'].x, torch.zeros(data['flow'].x.shape[0], 2)], dim=1)\n",
    "dashboard_obj = Module6Dashboard(\n",
    "    gnn_model=model,\n",
    "    rl_agent=model_rl,\n",
    "    pyg_graph=data,\n",
    "    rl_output=output,\n",
    "    predicted_kpis=predicted_kpis,\n",
    "    flow_feature_names=flow_feature_names\n",
    ")\n",
    "\n",
    "gnn_attr = dashboard_obj.gnn_explain(feature_idx=0)\n",
    "state_samples = []\n",
    "state, _ = env.reset()\n",
    "for i in range(len(output)):\n",
    "    state_samples.append(state)\n",
    "    action, _ = model_rl.predict(state)\n",
    "    state, _, terminated, _, _ = env.step(action)\n",
    "    if terminated:\n",
    "        break\n",
    "rl_shap = dashboard_obj.rl_explain(state_samples, nsamples=len(state_samples))\n",
    "\n",
    "dashboard_report = dashboard_obj.generate_dashboard(\n",
    "    gnn_attr=gnn_attr,\n",
    "    rl_shap=rl_shap,\n",
    "    top_features=3\n",
    ")\n",
    "print(\"\\n=== KPI + RL Dashboard Report ===\\n\")\n",
    "print(\"GNN Explanations:\")\n",
    "for msg in dashboard_report['gnn_explanations']:\n",
    "    print(msg)\n",
    "print(\"\\nRL Explanations:\")\n",
    "for msg in dashboard_report['rl_explanations']:\n",
    "    print(msg)\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "for k, v in dashboard_report['evaluation_metrics'].items():\n",
    "    print(f\"{k}: {v:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 784
    },
    "id": "q-Q1ID23Tey3",
    "outputId": "5ab857af-8dd5-4159-d393-ca26834e6690"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------\n",
    "# Visualization functions\n",
    "# ------------------------\n",
    "\n",
    "def plot_topology(G, metrics):\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw(G, pos, with_labels=True, node_color='skyblue', edge_color='gray', ax=ax)\n",
    "    ax.set_title(\"Module 2: Network Topology\")\n",
    "    return fig\n",
    "\n",
    "def plot_kpi(predicted_kpis):\n",
    "    fig, ax = plt.subplots(figsize=(8,4))\n",
    "    flows = list(range(len(predicted_kpis['delay'])))\n",
    "    ax.bar(flows, predicted_kpis['delay'], alpha=0.5, label='Delay')\n",
    "    ax.bar(flows, predicted_kpis['jitter'], alpha=0.5, label='Jitter')\n",
    "    ax.bar(flows, predicted_kpis['loss'], alpha=0.5, label='Loss')\n",
    "    ax.set_xlabel(\"Flow ID\"); ax.set_ylabel(\"KPI Value\"); ax.set_title(\"Module 3: Predicted KPIs\")\n",
    "    ax.legend()\n",
    "    return fig\n",
    "\n",
    "def plot_rl_decisions(rl_output):\n",
    "    fig, ax = plt.subplots(figsize=(8,4))\n",
    "    admits = [s['admit'] for s in rl_output]\n",
    "    allocations = [s['bandwidth_allocation'] for s in rl_output]\n",
    "    ax.bar(range(len(rl_output)), allocations, color=['green' if a else 'red' for a in admits])\n",
    "    ax.set_xlabel(\"Slice ID\"); ax.set_ylabel(\"Bandwidth Allocation\")\n",
    "    ax.set_title(\"Module 4: RL Slice Admission & Allocation\")\n",
    "    return fig\n",
    "\n",
    "def plot_slice_status(slice_status):\n",
    "    fig, ax = plt.subplots(figsize=(8,4))\n",
    "    delays = [s['current_metrics']['delay'] for s in slice_status]\n",
    "    jitt = [s['current_metrics']['jitter'] for s in slice_status]\n",
    "    losses = [s['current_metrics']['loss'] for s in slice_status]\n",
    "    ax.plot(delays, label='Delay')\n",
    "    ax.plot(jitt, label='Jitter')\n",
    "    ax.plot(losses, label='Loss')\n",
    "    ax.set_xlabel(\"Slice ID\"); ax.set_ylabel(\"KPI Value\")\n",
    "    ax.set_title(\"Module 5: Slice Lifecycle Status\")\n",
    "    ax.legend()\n",
    "    return fig\n",
    "\n",
    "def show_dashboard_structured(dashboard):\n",
    "    gnn_rows = []\n",
    "    for i, msg in enumerate(dashboard['gnn_explanations']):\n",
    "        gnn_rows.append({\"Flow ID\": i, \"Explanation\": msg})\n",
    "    gnn_df = pd.DataFrame(gnn_rows)\n",
    "\n",
    "    rl_rows = []\n",
    "    for i, msg in enumerate(dashboard['rl_explanations']):\n",
    "        rl_rows.append({\"Decision ID\": i, \"Explanation\": msg})\n",
    "    rl_df = pd.DataFrame(rl_rows)\n",
    "    metrics = dashboard['evaluation_metrics']\n",
    "    metrics_md = \"\\n\".join([f\"**{k.replace('_',' ').title()}:** {v:.3f}\" for k,v in metrics.items()])\n",
    "\n",
    "    return gnn_df, rl_df, metrics_md\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Gradio function\n",
    "# ------------------------\n",
    "\n",
    "def visualize_module(module_choice):\n",
    "    if module_choice == \"Module 2: Topology\":\n",
    "        return plot_topology(result['topology'], metrics)\n",
    "    elif module_choice == \"Module 3: KPI Prediction\":\n",
    "        return plot_kpi(predicted_kpis)\n",
    "    elif module_choice == \"Module 4: RL Decisions\":\n",
    "        return plot_rl_decisions(output)\n",
    "    elif module_choice == \"Module 5: Slice Status\":\n",
    "        return plot_slice_status(manager.get_status())\n",
    "    elif module_choice == \"Module 6: Dashboard\":\n",
    "        return show_dashboard(dashboard_report)\n",
    "\n",
    "# ------------------------\n",
    "# Gradio Interface\n",
    "# ------------------------\n",
    "\n",
    "module_options = [\n",
    "    \"Module 2: Topology\",\n",
    "    \"Module 3: KPI Prediction\",\n",
    "    \"Module 4: RL Decisions\",\n",
    "    \"Module 5: Slice Status\",\n",
    "    \"Module 6: Dashboard\"\n",
    "]\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    dropdown = gr.Dropdown(module_options, label=\"Select Module\")\n",
    "    output_plot = gr.Plot()\n",
    "    gnn_table = gr.Dataframe(headers=[\"Flow ID\",\"Explanation\"], interactive=False)\n",
    "    rl_table = gr.Dataframe(headers=[\"Decision ID\",\"Explanation\"], interactive=False)\n",
    "    metrics_md = gr.Markdown()\n",
    "\n",
    "\n",
    "    def update(module_choice):\n",
    "      if module_choice == \"Module 6: Dashboard\":\n",
    "          gnn_df, rl_df, metrics_text = show_dashboard_structured(dashboard_report)\n",
    "          return gr.update(visible=False), gr.update(value=gnn_df), gr.update(value=rl_df), gr.update(value=metrics_text)\n",
    "      else:\n",
    "          fig = visualize_module(module_choice)\n",
    "          return gr.update(visible=True, value=fig), gr.update(value=pd.DataFrame()), gr.update(value=pd.DataFrame()), gr.update(value=\"\")\n",
    "\n",
    "\n",
    "    dropdown.change(\n",
    "        fn=update,\n",
    "        inputs=dropdown,\n",
    "        outputs=[output_plot, gnn_table, rl_table, metrics_md]\n",
    "    )\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
